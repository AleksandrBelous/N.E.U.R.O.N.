{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:55:47.807197Z",
     "iopub.status.busy": "2024-11-13T19:55:47.806344Z",
     "iopub.status.idle": "2024-11-13T19:55:47.868894Z",
     "shell.execute_reply": "2024-11-13T19:55:47.868038Z",
     "shell.execute_reply.started": "2024-11-13T19:55:47.807171Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    silhouette_score,\n",
    "    homogeneity_score,\n",
    "    completeness_score,\n",
    "    v_measure_score,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T19:55:47.871279Z",
     "iopub.status.busy": "2024-11-13T19:55:47.870239Z",
     "iopub.status.idle": "2024-11-13T20:11:16.831563Z",
     "shell.execute_reply": "2024-11-13T20:11:16.830905Z",
     "shell.execute_reply.started": "2024-11-13T19:55:47.871252Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка файлов:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение на выборках в 10 пакетов\n",
      "  - Обучение модели Agglomerative Clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка файлов:  11%|█         | 1/9 [04:31<36:12, 271.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение на выборках в 50 пакетов\n",
      "  - Обучение модели Agglomerative Clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка файлов:  22%|██▏       | 2/9 [04:39<13:34, 116.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение на выборках в 100 пакетов\n",
      "  - Обучение модели Agglomerative Clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка файлов:  33%|███▎      | 3/9 [04:41<06:25, 64.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение на выборках в 250 пакетов\n",
      "  - Обучение модели Agglomerative Clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка файлов:  44%|████▍     | 4/9 [04:42<03:15, 39.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение на выборках в 500 пакетов\n",
      "  - Обучение модели Agglomerative Clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка файлов:  67%|██████▋   | 6/9 [04:42<00:49, 16.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение на выборках в 750 пакетов\n",
      "  - Обучение модели Agglomerative Clustering\n",
      "Обучение на выборках в 1000 пакетов\n",
      "  - Обучение модели Agglomerative Clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка файлов: 100%|██████████| 9/9 [04:42<00:00, 31.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение на выборках в 5000 пакетов\n",
      "  - Обучение модели Agglomerative Clustering\n",
      "Обучение на выборках в 10000 пакетов\n",
      "  - Обучение модели Agglomerative Clustering\n",
      "Итоговый файл сохранён в 'final_unsupervised_models_results.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Путь к объединенным CSV-файлам\n",
    "combined_path = \"../combined_csvs\"\n",
    "combined_files = [f for f in os.listdir(combined_path) if f.endswith(\".csv\")]\n",
    "\n",
    "# Сортировка файлов по числу пакетов (например, _250_)\n",
    "combined_files.sort(key=lambda f: int(re.search(r\"(\\d+)_packets\", f).group(1)))\n",
    "\n",
    "# Создание папки для промежуточных файлов\n",
    "intermediate_path = \"./unsupervised_models_results/\"\n",
    "os.makedirs(intermediate_path, exist_ok=True)\n",
    "\n",
    "# Итоговый список для всех результатов\n",
    "all_results = []\n",
    "\n",
    "# Проход по каждому CSV-файлу\n",
    "for combined_file in tqdm(combined_files, desc=\"Обработка файлов\"):\n",
    "    df = pd.read_csv(os.path.join(combined_path, combined_file))\n",
    "    X = df.drop(columns=[\"Label\"])\n",
    "    y_true = df[\"Label\"]\n",
    "\n",
    "    # Извлечение числа уникальных классов и размера пакетов из имени файла\n",
    "    n_classes = y_true.nunique()\n",
    "    packet_size = int(re.search(r\"(\\d+)_packets\", combined_file).group(1))\n",
    "\n",
    "    print(f\"Обучение на выборках в {packet_size} пакетов\")\n",
    "\n",
    "    # Нормализация данных\n",
    "    try:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "    except ValueError as e:\n",
    "        print(f\"  ! Ошибка при нормализации данных для файла '{combined_file}': {e}\")\n",
    "        continue\n",
    "\n",
    "    connectivity = kneighbors_graph(X_scaled, n_neighbors=10)\n",
    "\n",
    "    # Словарь моделей с учетом числа классов\n",
    "    models = {\n",
    "        # \"K-Means\": KMeans(n_clusters=n_classes, random_state=42),\n",
    "        # \"DBSCAN\": DBSCAN(eps=0.5, min_samples=10),\n",
    "        \"Agglomerative Clustering\": AgglomerativeClustering(\n",
    "            n_clusters=n_classes, connectivity=connectivity\n",
    "        ),\n",
    "        # \"Gaussian Mixture\": GaussianMixture(n_components=n_classes, random_state=42),\n",
    "        # \"Spectral Clustering\": SpectralClustering(n_clusters=n_classes, random_state=42),\n",
    "    }\n",
    "\n",
    "    # Проход по каждой модели\n",
    "    for model_name, model in models.items():\n",
    "        results = {\n",
    "            \"Model\": model_name,\n",
    "            \"File\": combined_file,\n",
    "            \"Packet Size\": packet_size,\n",
    "        }\n",
    "\n",
    "        print(f\"  - Обучение модели {model_name}\")\n",
    "        try:\n",
    "            # Обучение модели и предсказания\n",
    "            if hasattr(model, \"fit_predict\"):\n",
    "                y_pred = model.fit_predict(X_scaled)\n",
    "            else:\n",
    "                model.fit(X_scaled)\n",
    "                y_pred = model.predict(X_scaled)\n",
    "\n",
    "            # Расчёт метрик\n",
    "            results[\"Adjusted Rand Index\"] = adjusted_rand_score(y_true, y_pred)\n",
    "            results[\"Adjusted Mutual Information\"] = adjusted_mutual_info_score(\n",
    "                y_true, y_pred\n",
    "            )\n",
    "            results[\"Silhouette Score\"] = silhouette_score(X_scaled, y_pred)\n",
    "            results[\"Homogeneity\"] = homogeneity_score(y_true, y_pred)\n",
    "            results[\"Completeness\"] = completeness_score(y_true, y_pred)\n",
    "            results[\"V-Measure\"] = v_measure_score(y_true, y_pred)\n",
    "\n",
    "        except ValueError as e:\n",
    "            # Пропуск метрик при ошибке\n",
    "            results.update(\n",
    "                {\n",
    "                    \"Adjusted Rand Index\": \"0.0\",\n",
    "                    \"Adjusted Mutual Information\": \"0.0\",\n",
    "                    \"Silhouette Score\": \"0.0\",\n",
    "                    \"Homogeneity\": \"0.0\",\n",
    "                    \"Completeness\": \"0.0\",\n",
    "                    \"V-Measure\": \"0.0\",\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"  ! Ошибка обучения модели '{model_name}' для файла '{combined_file}': {e}\"\n",
    "            )\n",
    "\n",
    "        # Добавление результата\n",
    "        all_results.append(results)\n",
    "\n",
    "        # Промежуточный CSV для модели\n",
    "        model_results_df = pd.DataFrame([results])\n",
    "        model_results_file = os.path.join(\n",
    "            intermediate_path, f\"{model_name}_unsupervised_results.csv\"\n",
    "        )\n",
    "\n",
    "        # Если файл существует, добавляем данные\n",
    "        if os.path.exists(model_results_file):\n",
    "            model_results_df.to_csv(\n",
    "                model_results_file,\n",
    "                mode=\"a\",\n",
    "                index=False,\n",
    "                header=False,\n",
    "                float_format=\"%.3f\",\n",
    "            )\n",
    "        else:\n",
    "            model_results_df.to_csv(\n",
    "                model_results_file, index=False, float_format=\"%.3f\"\n",
    "            )\n",
    "\n",
    "# Итоговый файл с результатами всех моделей\n",
    "final_results_df = pd.DataFrame(all_results)\n",
    "final_results_df.to_csv(\n",
    "    \"final_unsupervised_models_results.csv\", index=False, float_format=\"%.3f\"\n",
    ")\n",
    "print(\"Итоговый файл сохранён в 'final_unsupervised_models_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T20:11:16.833090Z",
     "iopub.status.busy": "2024-11-13T20:11:16.832504Z",
     "iopub.status.idle": "2024-11-13T20:11:17.051061Z",
     "shell.execute_reply": "2024-11-13T20:11:17.050478Z",
     "shell.execute_reply.started": "2024-11-13T20:11:16.833066Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Создание папки для модифицированных промежуточных файлов\n",
    "intermediate_path_modified = \"./unsupervised_models_results_modified/\"\n",
    "os.makedirs(intermediate_path_modified, exist_ok=True)\n",
    "\n",
    "# Путь к промежуточным файлам\n",
    "intermediate_path = \"./unsupervised_models_results/\"\n",
    "model_results_files = [\n",
    "    f for f in os.listdir(intermediate_path) if f.endswith(\"_unsupervised_results.csv\")\n",
    "]\n",
    "\n",
    "# Создаем итоговый DataFrame для всех моделей\n",
    "final_results = []\n",
    "\n",
    "# Процесс обработки каждого промежуточного файла модели\n",
    "for model_results_file in model_results_files:\n",
    "    # Чтение промежуточного CSV файла\n",
    "    df = pd.read_csv(os.path.join(intermediate_path, model_results_file))\n",
    "    # display(df)\n",
    "\n",
    "    # Получаем список метрик, исключая \"Packet Size\" и \"Model\"\n",
    "    metrics_columns = [col for col in df.columns if col not in ['Packet Size', 'Model', 'File']]\n",
    "\n",
    "    # Определение лучшей выборки для каждой метрики\n",
    "    best = {}\n",
    "    counts = {}\n",
    "\n",
    "    # Добавление столбца Counts для каждой выборки\n",
    "    max1_counts_column = []\n",
    "    for index, row in df.iterrows():\n",
    "        max1_count = sum(1 for col in metrics_columns if row[col] == df[col].max())  # Считаем, сколько раз максимальное значение встречается\n",
    "        max1_counts_column.append(max1_count)\n",
    "\n",
    "    df[\"max 1\"] = max1_counts_column\n",
    "\n",
    "    # Для каждой метрики находим наилучшие выборки\n",
    "    for col in metrics_columns:  # Перебираем только метрики\n",
    "        max_value = df[col].max()  # Находим максимальное значение для метрики\n",
    "        best[col] = df[df[col] == max_value][\"Packet Size\"].tolist()  # Сохраняем размеры выборок с максимальным значением\n",
    "\n",
    "        # Подсчитаем, сколько раз каждая выборка была максимальной для этой метрики\n",
    "        counts[col] = df[col].value_counts().get(max_value, 0)  # Подсчитаем сколько раз максимальное значение встречается\n",
    "\n",
    "    # Строка Best\n",
    "    best_row = {\"Packet Size\": \"Best\"}\n",
    "    for col in metrics_columns:\n",
    "        best_row[col] = best[col]  # Присваиваем лучшие выборки для каждой метрики\n",
    "\n",
    "    # Добавление строки Best в DataFrame\n",
    "    df_best = pd.DataFrame([best_row])\n",
    "\n",
    "    # Добавление строки Best в DataFrame\n",
    "    df = pd.concat([df, df_best], ignore_index=True)\n",
    "    \n",
    "    model_results_file_modified = os.path.join(\n",
    "            intermediate_path_modified, f\"{os.path.splitext(model_results_file)[0]}_modified.csv\"\n",
    "        )\n",
    "\n",
    "    # Запись результатов в новый CSV файл\n",
    "    df.to_csv(model_results_file_modified, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-2-12",
   "language": "python",
   "name": "tf-2-12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
