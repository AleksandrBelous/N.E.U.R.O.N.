{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T10:26:26.790854Z",
     "start_time": "2025-03-30T10:26:25.995605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scapy.utils import rdpcap, PcapReader\n",
    "from scapy.layers.inet import IP, TCP, UDP"
   ],
   "id": "3a6c53f940601715",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Согласно статье, на вход модели подаются первые n пакетов каждого сетевого потока, причем:\n",
    "\n",
    "- Фильтрация: Выбираются пакеты, принадлежащие одному 5-тuple (IP-адреса, порты, протокол).\n",
    "\n",
    "- Анонимизация: MAC- и IP-адреса маскируются.\n",
    "\n",
    "- Трансформация: Каждый пакет приводится к фиксированной длине l (если пакет длиннее – усечение, если короче – дополнение нулями).\n",
    "\n",
    "- Нормализация: Значения байтов нормализуются (например, делением на 255).\n",
    "\n",
    "- Конкатенация: n пакетов объединяются в единый входной вектор.\n",
    "\n",
    "Пример кода для обработки может выглядеть следующим образом (упрощённо):"
   ],
   "id": "45aac96c9827ed8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T10:26:26.798389Z",
     "start_time": "2025-03-30T10:26:26.795819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_pcap_lazy(file_path):\n",
    "    \"\"\"\n",
    "    Эффективное чтение pcap-файла с помощью генератора.\n",
    "    \"\"\"\n",
    "    with PcapReader(file_path) as pcap_reader:\n",
    "        for packet in pcap_reader:\n",
    "            yield packet  # Возвращает пакеты по одному (ленивая загрузка)"
   ],
   "id": "b98e53ae7379455",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T10:26:26.921162Z",
     "start_time": "2025-03-30T10:26:26.918563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Функция для предварительной обработки одного пакета:\n",
    "def preprocess_packet(packet_bytes, l=100):\n",
    "    \"\"\"\n",
    "    Приводит пакет к фиксированной длине l.\n",
    "    Если пакет длиннее l, он усекается,\n",
    "    если короче — дополняется нулями.\n",
    "    Затем выполняется нормализация значений.\n",
    "    \"\"\"\n",
    "    pkt = np.frombuffer(packet_bytes, dtype=np.uint8)\n",
    "    pkt = pkt[:l] if len(pkt) > l else np.pad(pkt, (0, l - len(pkt)), mode='constant')\n",
    "    # Нормализация в диапазон [0, 1]\n",
    "    return pkt.astype(np.float32) / 255.0"
   ],
   "id": "89c028202f3e8ebf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-30T10:26:26.970764Z",
     "start_time": "2025-03-30T10:26:26.963146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def group_packets_by_flow(pcap_file, label, n_packets=2, l=100, flow_timeout=120):\n",
    "    \"\"\"\n",
    "    Загружает pcap-файл и группирует пакеты по потоку с учетом:\n",
    "    - Таймаута: если время между пакетами превышает flow_timeout, поток завершается.\n",
    "    - FIN-флага в TCP: поток завершается при обнаружении FIN-флага.\n",
    "    - Анонимизации MAC и IP адресов.\n",
    "    Для каждого потока выбираются первые n_packets, каждый пакет приводится к длине l,\n",
    "    после чего пакеты конкатенируются в единый входной вектор.\n",
    "    \"\"\"\n",
    "    flows = []  # Список завершенных потоков: (flow_key, [bytes пакетов])\n",
    "    active_flows = { }  # Словарь активных потоков: flow_key -> {'packets': [...], 'last_time': ...}\n",
    "\n",
    "    for pkt in read_pcap_lazy(pcap_file):\n",
    "        if not pkt.haslayer(IP):\n",
    "            continue\n",
    "\n",
    "        # Анонимизация MAC-адресов, если есть Ethernet-слой\n",
    "        if pkt.haslayer('Ether'):\n",
    "            pkt.src = \"00:00:00:00:00:00\"\n",
    "            pkt.dst = \"00:00:00:00:00:00\"\n",
    "\n",
    "        # Анонимизация IP-адресов\n",
    "        ip_layer = pkt[IP]\n",
    "        ip_layer.src = \"0.0.0.0\"\n",
    "        ip_layer.dst = \"0.0.0.0\"\n",
    "\n",
    "        # Извлечение портов в зависимости от протокола\n",
    "        sport, dport = None, None\n",
    "        if pkt.haslayer(TCP):\n",
    "            sport = pkt[TCP].sport\n",
    "            dport = pkt[TCP].dport\n",
    "        elif pkt.haslayer(UDP):\n",
    "            sport = pkt[UDP].sport\n",
    "            dport = pkt[UDP].dport\n",
    "\n",
    "        proto = ip_layer.proto\n",
    "        # Формирование ключа потока по 5-tuple\n",
    "        flow_key = (ip_layer.src, ip_layer.dst, sport, dport, proto)\n",
    "        pkt_time = pkt.time if hasattr(pkt, \"time\") else None\n",
    "\n",
    "        if flow_key in active_flows:\n",
    "            current_flow = active_flows[flow_key]\n",
    "            # Если временной промежуток превышает flow_timeout, завершаем текущий поток\n",
    "            if pkt_time is not None and (pkt_time - current_flow['last_time'] > flow_timeout):\n",
    "                flows.append((flow_key, current_flow['packets']))\n",
    "                active_flows[flow_key] = { 'packets': [], 'last_time': pkt_time }\n",
    "            else:\n",
    "                # Если пакет TCP и содержит FIN-флаг, то считаем поток завершенным\n",
    "                if pkt.haslayer(TCP):\n",
    "                    tcp_layer = pkt[TCP]\n",
    "                    if 'F' in tcp_layer.flags:\n",
    "                        current_flow['packets'].append(bytes(pkt))\n",
    "                        flows.append((flow_key, current_flow['packets']))\n",
    "                        active_flows[flow_key] = { 'packets': [], 'last_time': pkt_time }\n",
    "                        continue\n",
    "                # Иначе добавляем пакет в текущий поток\n",
    "                current_flow['packets'].append(bytes(pkt))\n",
    "                current_flow['last_time'] = pkt_time\n",
    "        else:\n",
    "            active_flows[flow_key] = { 'packets': [bytes(pkt)], 'last_time': pkt_time }\n",
    "\n",
    "    # Завершаем оставшиеся активные потоки\n",
    "    for key, flow in active_flows.items():\n",
    "        if flow['packets']:\n",
    "            flows.append((key, flow['packets']))\n",
    "\n",
    "    # Обработка и сохранение в CSV\n",
    "    flow_data = []\n",
    "    vec_size = n_packets * l\n",
    "    columns = [f'byte_{i}' for i in range(vec_size)] + ['label']\n",
    "\n",
    "    for _, pkts in flows:\n",
    "        if len(pkts) < n_packets:\n",
    "            continue\n",
    "        processed = [preprocess_packet(pkt, l) for pkt in pkts[:n_packets]]\n",
    "        vector = np.concatenate(processed).tolist()\n",
    "        vector.append(label)\n",
    "        flow_data.append(vector)\n",
    "\n",
    "    df = pd.DataFrame(flow_data, columns=columns)\n",
    "    # dataset_file = f\"{prefix}_n_packets={n_packets}_l={l}_label={label}.csv\"\n",
    "    # os.makedirs(os.path.dirname(dataset_file) or \".\", exist_ok=True)\n",
    "    # df.to_csv(dataset_file, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "    # # Для каждого потока выбираем первые n_packets, обрабатываем и конкатенируем\n",
    "    # flow_list = []\n",
    "    # for key, pkts in flows:\n",
    "    #     if len(pkts) < n_packets:\n",
    "    #         continue\n",
    "    #     processed_pkts = [preprocess_packet(pkt, l) for pkt in pkts[:n_packets]]\n",
    "    #     catenated_pkts = np.concatenate(processed_pkts)\n",
    "    #     print(catenated_pkts)\n",
    "    #     flow_list.append(catenated_pkts)\n",
    "    #\n",
    "    # dataset_file = f\"{prefix}_n_packets={n_packets}_l={l}_label={label}.npy\"\n",
    "    # # Создаем директорию, если требуется (dataset_file может содержать путь)\n",
    "    # os.makedirs(os.path.dirname(dataset_file) or \".\", exist_ok=True)\n",
    "    # np.save(dataset_file, flow_list)\n",
    "    #\n",
    "    # return np.array(flow_list)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T10:26:27.020737Z",
     "start_time": "2025-03-30T10:26:27.016531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_pcaps_in_folder(pcap_folder, output_csv, label, n_packets, l):\n",
    "    \"\"\"\n",
    "    Обрабатывает все PCAP-файлы в папке и сохраняет в один CSV.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(pcap_folder):\n",
    "        return\n",
    "\n",
    "    all_flows = []\n",
    "    for pcap_file in os.listdir(pcap_folder):\n",
    "        if pcap_file.endswith(\".pcap\"):\n",
    "            pcap_path = os.path.join(pcap_folder, pcap_file)\n",
    "            flows = group_packets_by_flow(pcap_file=pcap_path,\n",
    "                                          label=label,\n",
    "                                          n_packets=n_packets,\n",
    "                                          l=l,\n",
    "                                          )\n",
    "            if flows is not None:\n",
    "                all_flows.append(flows)\n",
    "\n",
    "    if all_flows:\n",
    "        final_df = pd.concat(all_flows, ignore_index=True)\n",
    "        final_df.to_csv(output_csv, index=False)\n",
    "        print(f\"Processed {pcap_folder} PCAP files and saved to {output_csv}\")\n"
   ],
   "id": "adc8fd284e14fbc",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T10:26:27.068228Z",
     "start_time": "2025-03-30T10:26:27.064772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_attack_category(base_attacks_dir, attack_name, output_root, n_packets, l):\n",
    "    \"\"\"\n",
    "    Обрабатывает все PCAP-файлы для конкретной атаки (attack/normal) и сохраняет CSV.\n",
    "    \"\"\"\n",
    "    attack_dir = os.path.join(base_attacks_dir, attack_name)\n",
    "    pcap_attack_dir = os.path.join(attack_dir, 'datasets_pcap', 'attack')\n",
    "    pcap_normal_dir = os.path.join(attack_dir, 'datasets_pcap', 'normal')\n",
    "\n",
    "    # Создаем выходную директорию для атаки: output_root/attack_name\n",
    "    output_dir = os.path.join(output_root, attack_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Обработка атакующих PCAP (label=0)\n",
    "    attack_csv = os.path.join(output_dir, f\"{attack_name}_attack.csv\")\n",
    "    process_pcaps_in_folder(pcap_attack_dir, attack_csv, label=1, n_packets=n_packets, l=l)\n",
    "\n",
    "    # Обработка нормальных PCAP (label=1)\n",
    "    normal_csv = os.path.join(output_dir, f\"{attack_name}_normal.csv\")\n",
    "    process_pcaps_in_folder(pcap_normal_dir, normal_csv, label=0, n_packets=n_packets, l=l)"
   ],
   "id": "ae3f21ec31187c4b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T18:12:45.997754Z",
     "start_time": "2025-03-30T18:05:24.407934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Параметры обработки\n",
    "N_PACKETS = 5\n",
    "L = 100\n",
    "\n",
    "# Директория с атаками\n",
    "BASE_ATTACKS_DIR = \"../CIC-IDS-2017/attacks\"\n",
    "\n",
    "# Выходная директория\n",
    "OUTPUT_ROOT = f\"n_packets={N_PACKETS}_l={L}\"\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "\n",
    "# Обработка всех категорий атак\n",
    "for attack_category in os.listdir(BASE_ATTACKS_DIR):\n",
    "    attack_path = os.path.join(BASE_ATTACKS_DIR, attack_category)\n",
    "    if os.path.isdir(attack_path):\n",
    "        process_attack_category(\n",
    "                base_attacks_dir=BASE_ATTACKS_DIR,\n",
    "                attack_name=attack_category,\n",
    "                output_root=OUTPUT_ROOT,\n",
    "                n_packets=N_PACKETS,\n",
    "                l=L,\n",
    "                )"
   ],
   "id": "5d0abe8f7250cb8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ../CIC-IDS-2017/attacks/brute-http/datasets_pcap/attack PCAP files and saved to n_packets=5_l=100/brute-http/brute-http_attack.csv\n",
      "Processed ../CIC-IDS-2017/attacks/brute-http/datasets_pcap/normal PCAP files and saved to n_packets=5_l=100/brute-http/brute-http_normal.csv\n",
      "Processed ../CIC-IDS-2017/attacks/brute-ftp/datasets_pcap/attack PCAP files and saved to n_packets=5_l=100/brute-ftp/brute-ftp_attack.csv\n",
      "Processed ../CIC-IDS-2017/attacks/brute-ftp/datasets_pcap/normal PCAP files and saved to n_packets=5_l=100/brute-ftp/brute-ftp_normal.csv\n",
      "Processed ../CIC-IDS-2017/attacks/inf-usb/datasets_pcap/attack PCAP files and saved to n_packets=5_l=100/inf-usb/inf-usb_attack.csv\n",
      "Processed ../CIC-IDS-2017/attacks/inf-usb/datasets_pcap/normal PCAP files and saved to n_packets=5_l=100/inf-usb/inf-usb_normal.csv\n",
      "Processed ../CIC-IDS-2017/attacks/xss/datasets_pcap/attack PCAP files and saved to n_packets=5_l=100/xss/xss_attack.csv\n",
      "Processed ../CIC-IDS-2017/attacks/xss/datasets_pcap/normal PCAP files and saved to n_packets=5_l=100/xss/xss_normal.csv\n",
      "Processed ../CIC-IDS-2017/attacks/brute-ssh/datasets_pcap/attack PCAP files and saved to n_packets=5_l=100/brute-ssh/brute-ssh_attack.csv\n",
      "Processed ../CIC-IDS-2017/attacks/brute-ssh/datasets_pcap/normal PCAP files and saved to n_packets=5_l=100/brute-ssh/brute-ssh_normal.csv\n",
      "Processed ../CIC-IDS-2017/attacks/inf-dropbox/datasets_pcap/attack PCAP files and saved to n_packets=5_l=100/inf-dropbox/inf-dropbox_attack.csv\n",
      "Processed ../CIC-IDS-2017/attacks/inf-dropbox/datasets_pcap/normal PCAP files and saved to n_packets=5_l=100/inf-dropbox/inf-dropbox_normal.csv\n",
      "Processed ../CIC-IDS-2017/attacks/sql-inj/datasets_pcap/attack PCAP files and saved to n_packets=5_l=100/sql-inj/sql-inj_attack.csv\n",
      "Processed ../CIC-IDS-2017/attacks/sql-inj/datasets_pcap/normal PCAP files and saved to n_packets=5_l=100/sql-inj/sql-inj_normal.csv\n",
      "Processed ../CIC-IDS-2017/attacks/botnet-ares/datasets_pcap/attack PCAP files and saved to n_packets=5_l=100/botnet-ares/botnet-ares_attack.csv\n",
      "Processed ../CIC-IDS-2017/attacks/botnet-ares/datasets_pcap/normal PCAP files and saved to n_packets=5_l=100/botnet-ares/botnet-ares_normal.csv\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
